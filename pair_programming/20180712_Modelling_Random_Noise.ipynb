{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make a response y of 200 random numbers. <br> \n",
    "Then generate 20 random features in an X to predict with. <br>\n",
    "Fit a linear model using both the 'Famous Equation' and the python built-ins. <br>\n",
    "Check the summary() output of the results from the python built-ins. Do you have any features with P>|t| less than 0.05? (Repeat the process until you have at least one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.rand(200)\n",
    "X = np.random.rand(200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.726</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   27.55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 12 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>2.34e-44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:59:26</td>     <th>  Log-Likelihood:    </th> <td> -34.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   109.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   180</td>      <th>  BIC:               </th> <td>   175.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    20</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.0139</td> <td>    0.077</td> <td>    0.180</td> <td> 0.857</td> <td>   -0.138</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>  <td>   -0.1141</td> <td>    0.081</td> <td>   -1.402</td> <td> 0.163</td> <td>   -0.275</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>  <td>    0.0939</td> <td>    0.081</td> <td>    1.156</td> <td> 0.249</td> <td>   -0.066</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>  <td>    0.0640</td> <td>    0.078</td> <td>    0.824</td> <td> 0.411</td> <td>   -0.089</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>  <td>    0.0532</td> <td>    0.077</td> <td>    0.694</td> <td> 0.489</td> <td>   -0.098</td> <td>    0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>  <td>    0.0790</td> <td>    0.080</td> <td>    0.990</td> <td> 0.324</td> <td>   -0.079</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>  <td>    0.1252</td> <td>    0.075</td> <td>    1.673</td> <td> 0.096</td> <td>   -0.022</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>  <td>    0.0299</td> <td>    0.075</td> <td>    0.399</td> <td> 0.690</td> <td>   -0.118</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>  <td>   -0.0024</td> <td>    0.078</td> <td>   -0.030</td> <td> 0.976</td> <td>   -0.156</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th> <td>   -0.0014</td> <td>    0.080</td> <td>   -0.017</td> <td> 0.986</td> <td>   -0.159</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th> <td>    0.0602</td> <td>    0.076</td> <td>    0.795</td> <td> 0.428</td> <td>   -0.089</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th> <td>    0.0125</td> <td>    0.076</td> <td>    0.166</td> <td> 0.869</td> <td>   -0.137</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th> <td>    0.0662</td> <td>    0.079</td> <td>    0.842</td> <td> 0.401</td> <td>   -0.089</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th> <td>    0.1052</td> <td>    0.073</td> <td>    1.444</td> <td> 0.150</td> <td>   -0.039</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th> <td>    0.1341</td> <td>    0.076</td> <td>    1.764</td> <td> 0.079</td> <td>   -0.016</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th> <td>    0.1492</td> <td>    0.074</td> <td>    2.028</td> <td> 0.044</td> <td>    0.004</td> <td>    0.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th> <td>    0.0920</td> <td>    0.072</td> <td>    1.275</td> <td> 0.204</td> <td>   -0.050</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th> <td>   -0.0193</td> <td>    0.075</td> <td>   -0.258</td> <td> 0.797</td> <td>   -0.167</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th> <td>    0.0210</td> <td>    0.075</td> <td>    0.281</td> <td> 0.779</td> <td>   -0.126</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th> <td>    0.0315</td> <td>    0.076</td> <td>    0.416</td> <td> 0.678</td> <td>   -0.118</td> <td>    0.181</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>59.526</td> <th>  Durbin-Watson:     </th> <td>   1.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  10.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.040</td> <th>  Prob(JB):          </th> <td> 0.00538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.883</td> <th>  Cond. No.          </th> <td>    10.5</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.754\n",
       "Model:                            OLS   Adj. R-squared:                  0.726\n",
       "Method:                 Least Squares   F-statistic:                     27.55\n",
       "Date:                Thu, 12 Jul 2018   Prob (F-statistic):           2.34e-44\n",
       "Time:                        09:59:26   Log-Likelihood:                -34.927\n",
       "No. Observations:                 200   AIC:                             109.9\n",
       "Df Residuals:                     180   BIC:                             175.8\n",
       "Df Model:                          20                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.0139      0.077      0.180      0.857      -0.138       0.166\n",
       "x2            -0.1141      0.081     -1.402      0.163      -0.275       0.047\n",
       "x3             0.0939      0.081      1.156      0.249      -0.066       0.254\n",
       "x4             0.0640      0.078      0.824      0.411      -0.089       0.217\n",
       "x5             0.0532      0.077      0.694      0.489      -0.098       0.205\n",
       "x6             0.0790      0.080      0.990      0.324      -0.079       0.237\n",
       "x7             0.1252      0.075      1.673      0.096      -0.022       0.273\n",
       "x8             0.0299      0.075      0.399      0.690      -0.118       0.178\n",
       "x9            -0.0024      0.078     -0.030      0.976      -0.156       0.152\n",
       "x10           -0.0014      0.080     -0.017      0.986      -0.159       0.156\n",
       "x11            0.0602      0.076      0.795      0.428      -0.089       0.210\n",
       "x12            0.0125      0.076      0.166      0.869      -0.137       0.162\n",
       "x13            0.0662      0.079      0.842      0.401      -0.089       0.222\n",
       "x14            0.1052      0.073      1.444      0.150      -0.039       0.249\n",
       "x15            0.1341      0.076      1.764      0.079      -0.016       0.284\n",
       "x16            0.1492      0.074      2.028      0.044       0.004       0.294\n",
       "x17            0.0920      0.072      1.275      0.204      -0.050       0.234\n",
       "x18           -0.0193      0.075     -0.258      0.797      -0.167       0.129\n",
       "x19            0.0210      0.075      0.281      0.779      -0.126       0.168\n",
       "x20            0.0315      0.076      0.416      0.678      -0.118       0.181\n",
       "==============================================================================\n",
       "Omnibus:                       59.526   Durbin-Watson:                   1.920\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.452\n",
       "Skew:                           0.040   Prob(JB):                      0.00538\n",
       "Kurtosis:                       1.883   Cond. No.                         10.5\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(y,X)\n",
    "# Fit your model to your training set\n",
    "fit = model.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0138812 ,  0.11413487,  0.0939214 ,  0.0639605 ,  0.05321956,\n",
       "        0.07901027,  0.12516016,  0.02991872,  0.00237986,  0.00137289,\n",
       "        0.06022367,  0.01254908,  0.06624355,  0.10515101,  0.13409898,\n",
       "        0.14917142,  0.09196885,  0.01934175,  0.02097764,  0.03146381])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef1 = fit.params\n",
    "len(coef1[np.absolute(coef1) < 0.05])\n",
    "np.absolute(coef1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef2 = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0138812 ,  0.11413487,  0.0939214 ,  0.0639605 ,  0.05321956,\n",
       "        0.07901027,  0.12516016,  0.02991872,  0.00237986,  0.00137289,\n",
       "        0.06022367,  0.01254908,  0.06624355,  0.10515101,  0.13409898,\n",
       "        0.14917142,  0.09196885,  0.01934175,  0.02097764,  0.03146381])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.absolute(coef2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 12 Jul 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.472</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:15:25</td>     <th>  Log-Likelihood:    </th> <td> -19.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   120.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   159</td>      <th>  BIC:               </th> <td>   255.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    40</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.4274</td> <td>    0.251</td> <td>    1.702</td> <td> 0.091</td> <td>   -0.069</td> <td>    0.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1083</td> <td>    0.087</td> <td>   -1.240</td> <td> 0.217</td> <td>   -0.281</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.1003</td> <td>    0.082</td> <td>    1.225</td> <td> 0.222</td> <td>   -0.061</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0549</td> <td>    0.080</td> <td>   -0.684</td> <td> 0.495</td> <td>   -0.213</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0009</td> <td>    0.083</td> <td>    0.011</td> <td> 0.991</td> <td>   -0.162</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0378</td> <td>    0.083</td> <td>    0.455</td> <td> 0.649</td> <td>   -0.126</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.0229</td> <td>    0.087</td> <td>   -0.263</td> <td> 0.793</td> <td>   -0.195</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.0553</td> <td>    0.092</td> <td>    0.598</td> <td> 0.551</td> <td>   -0.127</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.1003</td> <td>    0.083</td> <td>   -1.210</td> <td> 0.228</td> <td>   -0.264</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0604</td> <td>    0.087</td> <td>    0.694</td> <td> 0.489</td> <td>   -0.112</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.1507</td> <td>    0.087</td> <td>   -1.736</td> <td> 0.084</td> <td>   -0.322</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.0250</td> <td>    0.079</td> <td>    0.315</td> <td> 0.753</td> <td>   -0.132</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.0272</td> <td>    0.078</td> <td>    0.351</td> <td> 0.726</td> <td>   -0.126</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0092</td> <td>    0.086</td> <td>   -0.107</td> <td> 0.915</td> <td>   -0.180</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.0497</td> <td>    0.079</td> <td>   -0.632</td> <td> 0.528</td> <td>   -0.205</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.0835</td> <td>    0.080</td> <td>   -1.045</td> <td> 0.297</td> <td>   -0.241</td> <td>    0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.2771</td> <td>    0.084</td> <td>    3.292</td> <td> 0.001</td> <td>    0.111</td> <td>    0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.1390</td> <td>    0.078</td> <td>   -1.779</td> <td> 0.077</td> <td>   -0.293</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.0885</td> <td>    0.083</td> <td>    1.072</td> <td> 0.285</td> <td>   -0.075</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0903</td> <td>    0.083</td> <td>    1.087</td> <td> 0.279</td> <td>   -0.074</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.0736</td> <td>    0.079</td> <td>   -0.928</td> <td> 0.355</td> <td>   -0.230</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.1660</td> <td>    0.081</td> <td>    2.047</td> <td> 0.042</td> <td>    0.006</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.1047</td> <td>    0.086</td> <td>    1.221</td> <td> 0.224</td> <td>   -0.065</td> <td>    0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.0237</td> <td>    0.081</td> <td>    0.292</td> <td> 0.771</td> <td>   -0.137</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>   -0.0564</td> <td>    0.081</td> <td>   -0.694</td> <td> 0.489</td> <td>   -0.217</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.0578</td> <td>    0.078</td> <td>   -0.744</td> <td> 0.458</td> <td>   -0.211</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0754</td> <td>    0.081</td> <td>   -0.931</td> <td> 0.353</td> <td>   -0.235</td> <td>    0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.0414</td> <td>    0.080</td> <td>   -0.519</td> <td> 0.604</td> <td>   -0.199</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.0817</td> <td>    0.078</td> <td>   -1.045</td> <td> 0.297</td> <td>   -0.236</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>    0.0518</td> <td>    0.080</td> <td>    0.647</td> <td> 0.518</td> <td>   -0.106</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.0226</td> <td>    0.080</td> <td>   -0.283</td> <td> 0.778</td> <td>   -0.181</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.0988</td> <td>    0.082</td> <td>    1.198</td> <td> 0.233</td> <td>   -0.064</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    0.0151</td> <td>    0.085</td> <td>    0.178</td> <td> 0.859</td> <td>   -0.153</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>   -0.0038</td> <td>    0.081</td> <td>   -0.046</td> <td> 0.963</td> <td>   -0.164</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -0.1066</td> <td>    0.081</td> <td>   -1.318</td> <td> 0.189</td> <td>   -0.266</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.0771</td> <td>    0.083</td> <td>    0.925</td> <td> 0.356</td> <td>   -0.088</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0079</td> <td>    0.080</td> <td>   -0.098</td> <td> 0.922</td> <td>   -0.167</td> <td>    0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.1068</td> <td>    0.080</td> <td>    1.332</td> <td> 0.185</td> <td>   -0.052</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0418</td> <td>    0.088</td> <td>    0.476</td> <td> 0.635</td> <td>   -0.132</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -0.0229</td> <td>    0.087</td> <td>   -0.264</td> <td> 0.792</td> <td>   -0.194</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>   -0.0624</td> <td>    0.077</td> <td>   -0.810</td> <td> 0.419</td> <td>   -0.215</td> <td>    0.090</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13.284</td> <th>  Durbin-Watson:     </th> <td>   2.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>   5.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.064</td> <th>  Prob(JB):          </th> <td>  0.0696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.211</td> <th>  Cond. No.          </th> <td>    42.2</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.202\n",
       "Model:                            OLS   Adj. R-squared:                  0.001\n",
       "Method:                 Least Squares   F-statistic:                     1.005\n",
       "Date:                Thu, 12 Jul 2018   Prob (F-statistic):              0.472\n",
       "Time:                        09:15:25   Log-Likelihood:                -19.065\n",
       "No. Observations:                 200   AIC:                             120.1\n",
       "Df Residuals:                     159   BIC:                             255.4\n",
       "Df Model:                          40                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.4274      0.251      1.702      0.091      -0.069       0.923\n",
       "x1            -0.1083      0.087     -1.240      0.217      -0.281       0.064\n",
       "x2             0.1003      0.082      1.225      0.222      -0.061       0.262\n",
       "x3            -0.0549      0.080     -0.684      0.495      -0.213       0.104\n",
       "x4             0.0009      0.083      0.011      0.991      -0.162       0.164\n",
       "x5             0.0378      0.083      0.455      0.649      -0.126       0.202\n",
       "x6            -0.0229      0.087     -0.263      0.793      -0.195       0.149\n",
       "x7             0.0553      0.092      0.598      0.551      -0.127       0.238\n",
       "x8            -0.1003      0.083     -1.210      0.228      -0.264       0.063\n",
       "x9             0.0604      0.087      0.694      0.489      -0.112       0.232\n",
       "x10           -0.1507      0.087     -1.736      0.084      -0.322       0.021\n",
       "x11            0.0250      0.079      0.315      0.753      -0.132       0.182\n",
       "x12            0.0272      0.078      0.351      0.726      -0.126       0.180\n",
       "x13           -0.0092      0.086     -0.107      0.915      -0.180       0.161\n",
       "x14           -0.0497      0.079     -0.632      0.528      -0.205       0.106\n",
       "x15           -0.0835      0.080     -1.045      0.297      -0.241       0.074\n",
       "x16            0.2771      0.084      3.292      0.001       0.111       0.443\n",
       "x17           -0.1390      0.078     -1.779      0.077      -0.293       0.015\n",
       "x18            0.0885      0.083      1.072      0.285      -0.075       0.252\n",
       "x19            0.0903      0.083      1.087      0.279      -0.074       0.254\n",
       "x20           -0.0736      0.079     -0.928      0.355      -0.230       0.083\n",
       "x21            0.1660      0.081      2.047      0.042       0.006       0.326\n",
       "x22            0.1047      0.086      1.221      0.224      -0.065       0.274\n",
       "x23            0.0237      0.081      0.292      0.771      -0.137       0.184\n",
       "x24           -0.0564      0.081     -0.694      0.489      -0.217       0.104\n",
       "x25           -0.0578      0.078     -0.744      0.458      -0.211       0.096\n",
       "x26           -0.0754      0.081     -0.931      0.353      -0.235       0.085\n",
       "x27           -0.0414      0.080     -0.519      0.604      -0.199       0.116\n",
       "x28           -0.0817      0.078     -1.045      0.297      -0.236       0.073\n",
       "x29            0.0518      0.080      0.647      0.518      -0.106       0.210\n",
       "x30           -0.0226      0.080     -0.283      0.778      -0.181       0.136\n",
       "x31            0.0988      0.082      1.198      0.233      -0.064       0.262\n",
       "x32            0.0151      0.085      0.178      0.859      -0.153       0.183\n",
       "x33           -0.0038      0.081     -0.046      0.963      -0.164       0.157\n",
       "x34           -0.1066      0.081     -1.318      0.189      -0.266       0.053\n",
       "x35            0.0771      0.083      0.925      0.356      -0.088       0.242\n",
       "x36           -0.0079      0.080     -0.098      0.922      -0.167       0.151\n",
       "x37            0.1068      0.080      1.332      0.185      -0.052       0.265\n",
       "x38            0.0418      0.088      0.476      0.635      -0.132       0.215\n",
       "x39           -0.0229      0.087     -0.264      0.792      -0.194       0.148\n",
       "x40           -0.0624      0.077     -0.810      0.419      -0.215       0.090\n",
       "==============================================================================\n",
       "Omnibus:                       13.284   Durbin-Watson:                   2.187\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):                5.330\n",
       "Skew:                           0.064   Prob(JB):                       0.0696\n",
       "Kurtosis:                       2.211   Cond. No.                         42.2\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X40 = np.random.rand(200, 40)\n",
    "model40 = sm.OLS(y, sm.add_constant(X40))\n",
    "# Fit your model to your training set\n",
    "fit40 = model40.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit40.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 12 Jul 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.177</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:16:12</td>     <th>  Log-Likelihood:    </th> <td> 0.53328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   120.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   139</td>      <th>  BIC:               </th> <td>   322.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    60</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.7333</td> <td>    0.318</td> <td>    2.303</td> <td> 0.023</td> <td>    0.104</td> <td>    1.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.0515</td> <td>    0.087</td> <td>   -0.590</td> <td> 0.556</td> <td>   -0.224</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0848</td> <td>    0.085</td> <td>   -1.000</td> <td> 0.319</td> <td>   -0.253</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0415</td> <td>    0.087</td> <td>   -0.475</td> <td> 0.636</td> <td>   -0.214</td> <td>    0.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0211</td> <td>    0.083</td> <td>   -0.254</td> <td> 0.800</td> <td>   -0.186</td> <td>    0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0385</td> <td>    0.081</td> <td>   -0.476</td> <td> 0.635</td> <td>   -0.199</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.1092</td> <td>    0.078</td> <td>   -1.408</td> <td> 0.161</td> <td>   -0.263</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.0846</td> <td>    0.090</td> <td>   -0.940</td> <td> 0.349</td> <td>   -0.263</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.1268</td> <td>    0.087</td> <td>   -1.463</td> <td> 0.146</td> <td>   -0.298</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0865</td> <td>    0.082</td> <td>    1.053</td> <td> 0.294</td> <td>   -0.076</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0737</td> <td>    0.080</td> <td>    0.920</td> <td> 0.359</td> <td>   -0.085</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0291</td> <td>    0.083</td> <td>   -0.350</td> <td> 0.727</td> <td>   -0.193</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.1103</td> <td>    0.087</td> <td>   -1.275</td> <td> 0.204</td> <td>   -0.281</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    0.0865</td> <td>    0.086</td> <td>    1.008</td> <td> 0.315</td> <td>   -0.083</td> <td>    0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.0457</td> <td>    0.087</td> <td>    0.527</td> <td> 0.599</td> <td>   -0.126</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.1045</td> <td>    0.085</td> <td>    1.226</td> <td> 0.222</td> <td>   -0.064</td> <td>    0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.1687</td> <td>    0.080</td> <td>   -2.100</td> <td> 0.038</td> <td>   -0.328</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.0515</td> <td>    0.083</td> <td>   -0.622</td> <td> 0.535</td> <td>   -0.215</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.0137</td> <td>    0.087</td> <td>   -0.158</td> <td> 0.875</td> <td>   -0.185</td> <td>    0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>   -0.0058</td> <td>    0.085</td> <td>   -0.068</td> <td> 0.946</td> <td>   -0.175</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.2236</td> <td>    0.087</td> <td>   -2.564</td> <td> 0.011</td> <td>   -0.396</td> <td>   -0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.0047</td> <td>    0.086</td> <td>   -0.054</td> <td> 0.957</td> <td>   -0.175</td> <td>    0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>   -0.1314</td> <td>    0.086</td> <td>   -1.528</td> <td> 0.129</td> <td>   -0.301</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.0318</td> <td>    0.079</td> <td>   -0.405</td> <td> 0.686</td> <td>   -0.187</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>   -0.0776</td> <td>    0.084</td> <td>   -0.925</td> <td> 0.357</td> <td>   -0.244</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.0259</td> <td>    0.085</td> <td>   -0.304</td> <td> 0.761</td> <td>   -0.194</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.1020</td> <td>    0.088</td> <td>   -1.159</td> <td> 0.249</td> <td>   -0.276</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.0585</td> <td>    0.082</td> <td>   -0.710</td> <td> 0.479</td> <td>   -0.221</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.1071</td> <td>    0.085</td> <td>   -1.255</td> <td> 0.212</td> <td>   -0.276</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>    0.0445</td> <td>    0.087</td> <td>    0.512</td> <td> 0.609</td> <td>   -0.127</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>    0.0614</td> <td>    0.085</td> <td>    0.724</td> <td> 0.470</td> <td>   -0.106</td> <td>    0.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>   -0.0740</td> <td>    0.085</td> <td>   -0.872</td> <td> 0.385</td> <td>   -0.242</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    0.0427</td> <td>    0.082</td> <td>    0.518</td> <td> 0.605</td> <td>   -0.120</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.1681</td> <td>    0.084</td> <td>    2.013</td> <td> 0.046</td> <td>    0.003</td> <td>    0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -0.1397</td> <td>    0.080</td> <td>   -1.741</td> <td> 0.084</td> <td>   -0.298</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.1167</td> <td>    0.085</td> <td>    1.379</td> <td> 0.170</td> <td>   -0.051</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0997</td> <td>    0.087</td> <td>   -1.144</td> <td> 0.255</td> <td>   -0.272</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.2359</td> <td>    0.091</td> <td>    2.597</td> <td> 0.010</td> <td>    0.056</td> <td>    0.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>   -0.0388</td> <td>    0.089</td> <td>   -0.437</td> <td> 0.663</td> <td>   -0.214</td> <td>    0.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -0.0793</td> <td>    0.084</td> <td>   -0.949</td> <td> 0.344</td> <td>   -0.245</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.0964</td> <td>    0.088</td> <td>    1.090</td> <td> 0.277</td> <td>   -0.078</td> <td>    0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.0618</td> <td>    0.083</td> <td>    0.745</td> <td> 0.458</td> <td>   -0.102</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    0.2304</td> <td>    0.087</td> <td>    2.652</td> <td> 0.009</td> <td>    0.059</td> <td>    0.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    0.0277</td> <td>    0.084</td> <td>    0.331</td> <td> 0.741</td> <td>   -0.138</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>    0.0536</td> <td>    0.087</td> <td>    0.618</td> <td> 0.538</td> <td>   -0.118</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.1496</td> <td>    0.091</td> <td>   -1.652</td> <td> 0.101</td> <td>   -0.329</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>    0.0412</td> <td>    0.083</td> <td>    0.496</td> <td> 0.621</td> <td>   -0.123</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.0207</td> <td>    0.089</td> <td>   -0.232</td> <td> 0.817</td> <td>   -0.197</td> <td>    0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>   -0.1373</td> <td>    0.082</td> <td>   -1.674</td> <td> 0.096</td> <td>   -0.299</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    0.0949</td> <td>    0.084</td> <td>    1.133</td> <td> 0.259</td> <td>   -0.071</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   -0.1134</td> <td>    0.085</td> <td>   -1.333</td> <td> 0.185</td> <td>   -0.282</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>   -0.1347</td> <td>    0.088</td> <td>   -1.532</td> <td> 0.128</td> <td>   -0.309</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>   -0.0819</td> <td>    0.087</td> <td>   -0.940</td> <td> 0.349</td> <td>   -0.254</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.1245</td> <td>    0.084</td> <td>    1.478</td> <td> 0.142</td> <td>   -0.042</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>    0.0947</td> <td>    0.092</td> <td>    1.032</td> <td> 0.304</td> <td>   -0.087</td> <td>    0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>    0.0046</td> <td>    0.081</td> <td>    0.057</td> <td> 0.955</td> <td>   -0.155</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>    0.0915</td> <td>    0.086</td> <td>    1.063</td> <td> 0.290</td> <td>   -0.079</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>    0.0861</td> <td>    0.087</td> <td>    0.988</td> <td> 0.325</td> <td>   -0.086</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>    0.0461</td> <td>    0.089</td> <td>    0.515</td> <td> 0.607</td> <td>   -0.131</td> <td>    0.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>    0.0270</td> <td>    0.082</td> <td>    0.327</td> <td> 0.744</td> <td>   -0.136</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>    0.1242</td> <td>    0.086</td> <td>    1.442</td> <td> 0.151</td> <td>   -0.046</td> <td>    0.294</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.050</td> <th>  Durbin-Watson:     </th> <td>   2.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.132</td> <th>  Jarque-Bera (JB):  </th> <td>   2.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.019</td> <th>  Prob(JB):          </th> <td>   0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.448</td> <th>  Cond. No.          </th> <td>    65.6</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.344\n",
       "Model:                            OLS   Adj. R-squared:                  0.061\n",
       "Method:                 Least Squares   F-statistic:                     1.214\n",
       "Date:                Thu, 12 Jul 2018   Prob (F-statistic):              0.177\n",
       "Time:                        09:16:12   Log-Likelihood:                0.53328\n",
       "No. Observations:                 200   AIC:                             120.9\n",
       "Df Residuals:                     139   BIC:                             322.1\n",
       "Df Model:                          60                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.7333      0.318      2.303      0.023       0.104       1.363\n",
       "x1            -0.0515      0.087     -0.590      0.556      -0.224       0.121\n",
       "x2            -0.0848      0.085     -1.000      0.319      -0.253       0.083\n",
       "x3            -0.0415      0.087     -0.475      0.636      -0.214       0.131\n",
       "x4            -0.0211      0.083     -0.254      0.800      -0.186       0.144\n",
       "x5            -0.0385      0.081     -0.476      0.635      -0.199       0.122\n",
       "x6            -0.1092      0.078     -1.408      0.161      -0.263       0.044\n",
       "x7            -0.0846      0.090     -0.940      0.349      -0.263       0.093\n",
       "x8            -0.1268      0.087     -1.463      0.146      -0.298       0.045\n",
       "x9             0.0865      0.082      1.053      0.294      -0.076       0.249\n",
       "x10            0.0737      0.080      0.920      0.359      -0.085       0.232\n",
       "x11           -0.0291      0.083     -0.350      0.727      -0.193       0.135\n",
       "x12           -0.1103      0.087     -1.275      0.204      -0.281       0.061\n",
       "x13            0.0865      0.086      1.008      0.315      -0.083       0.256\n",
       "x14            0.0457      0.087      0.527      0.599      -0.126       0.217\n",
       "x15            0.1045      0.085      1.226      0.222      -0.064       0.273\n",
       "x16           -0.1687      0.080     -2.100      0.038      -0.328      -0.010\n",
       "x17           -0.0515      0.083     -0.622      0.535      -0.215       0.112\n",
       "x18           -0.0137      0.087     -0.158      0.875      -0.185       0.158\n",
       "x19           -0.0058      0.085     -0.068      0.946      -0.175       0.163\n",
       "x20           -0.2236      0.087     -2.564      0.011      -0.396      -0.051\n",
       "x21           -0.0047      0.086     -0.054      0.957      -0.175       0.165\n",
       "x22           -0.1314      0.086     -1.528      0.129      -0.301       0.039\n",
       "x23           -0.0318      0.079     -0.405      0.686      -0.187       0.124\n",
       "x24           -0.0776      0.084     -0.925      0.357      -0.244       0.088\n",
       "x25           -0.0259      0.085     -0.304      0.761      -0.194       0.142\n",
       "x26           -0.1020      0.088     -1.159      0.249      -0.276       0.072\n",
       "x27           -0.0585      0.082     -0.710      0.479      -0.221       0.104\n",
       "x28           -0.1071      0.085     -1.255      0.212      -0.276       0.062\n",
       "x29            0.0445      0.087      0.512      0.609      -0.127       0.216\n",
       "x30            0.0614      0.085      0.724      0.470      -0.106       0.229\n",
       "x31           -0.0740      0.085     -0.872      0.385      -0.242       0.094\n",
       "x32            0.0427      0.082      0.518      0.605      -0.120       0.206\n",
       "x33            0.1681      0.084      2.013      0.046       0.003       0.333\n",
       "x34           -0.1397      0.080     -1.741      0.084      -0.298       0.019\n",
       "x35            0.1167      0.085      1.379      0.170      -0.051       0.284\n",
       "x36           -0.0997      0.087     -1.144      0.255      -0.272       0.073\n",
       "x37            0.2359      0.091      2.597      0.010       0.056       0.416\n",
       "x38           -0.0388      0.089     -0.437      0.663      -0.214       0.137\n",
       "x39           -0.0793      0.084     -0.949      0.344      -0.245       0.086\n",
       "x40            0.0964      0.088      1.090      0.277      -0.078       0.271\n",
       "x41            0.0618      0.083      0.745      0.458      -0.102       0.226\n",
       "x42            0.2304      0.087      2.652      0.009       0.059       0.402\n",
       "x43            0.0277      0.084      0.331      0.741      -0.138       0.193\n",
       "x44            0.0536      0.087      0.618      0.538      -0.118       0.225\n",
       "x45           -0.1496      0.091     -1.652      0.101      -0.329       0.029\n",
       "x46            0.0412      0.083      0.496      0.621      -0.123       0.206\n",
       "x47           -0.0207      0.089     -0.232      0.817      -0.197       0.155\n",
       "x48           -0.1373      0.082     -1.674      0.096      -0.299       0.025\n",
       "x49            0.0949      0.084      1.133      0.259      -0.071       0.260\n",
       "x50           -0.1134      0.085     -1.333      0.185      -0.282       0.055\n",
       "x51           -0.1347      0.088     -1.532      0.128      -0.309       0.039\n",
       "x52           -0.0819      0.087     -0.940      0.349      -0.254       0.090\n",
       "x53            0.1245      0.084      1.478      0.142      -0.042       0.291\n",
       "x54            0.0947      0.092      1.032      0.304      -0.087       0.276\n",
       "x55            0.0046      0.081      0.057      0.955      -0.155       0.164\n",
       "x56            0.0915      0.086      1.063      0.290      -0.079       0.262\n",
       "x57            0.0861      0.087      0.988      0.325      -0.086       0.258\n",
       "x58            0.0461      0.089      0.515      0.607      -0.131       0.223\n",
       "x59            0.0270      0.082      0.327      0.744      -0.136       0.190\n",
       "x60            0.1242      0.086      1.442      0.151      -0.046       0.294\n",
       "==============================================================================\n",
       "Omnibus:                        4.050   Durbin-Watson:                   2.046\n",
       "Prob(Omnibus):                  0.132   Jarque-Bera (JB):                2.547\n",
       "Skew:                          -0.019   Prob(JB):                        0.280\n",
       "Kurtosis:                       2.448   Cond. No.                         65.6\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X60 = np.random.rand(200, 60)\n",
    "model60 = sm.OLS(y, sm.add_constant(X60))\n",
    "# Fit your model to your training set\n",
    "fit60 = model60.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit60.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.7021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 12 Jul 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.954</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:16:34</td>     <th>  Log-Likelihood:    </th> <td> -2.9463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   167.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   119</td>      <th>  BIC:               </th> <td>   435.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    80</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3808</td> <td>    0.479</td> <td>    0.795</td> <td> 0.428</td> <td>   -0.568</td> <td>    1.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0179</td> <td>    0.102</td> <td>    0.176</td> <td> 0.860</td> <td>   -0.183</td> <td>    0.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0327</td> <td>    0.104</td> <td>   -0.316</td> <td> 0.753</td> <td>   -0.238</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0615</td> <td>    0.104</td> <td>   -0.589</td> <td> 0.557</td> <td>   -0.268</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.1137</td> <td>    0.097</td> <td>    1.176</td> <td> 0.242</td> <td>   -0.078</td> <td>    0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0642</td> <td>    0.104</td> <td>   -0.620</td> <td> 0.536</td> <td>   -0.269</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0609</td> <td>    0.102</td> <td>    0.596</td> <td> 0.552</td> <td>   -0.141</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.0276</td> <td>    0.108</td> <td>    0.256</td> <td> 0.798</td> <td>   -0.186</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.1214</td> <td>    0.103</td> <td>   -1.177</td> <td> 0.242</td> <td>   -0.326</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0090</td> <td>    0.110</td> <td>    0.082</td> <td> 0.935</td> <td>   -0.208</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.0049</td> <td>    0.094</td> <td>   -0.052</td> <td> 0.958</td> <td>   -0.192</td> <td>    0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.0132</td> <td>    0.102</td> <td>    0.129</td> <td> 0.897</td> <td>   -0.189</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.0567</td> <td>    0.093</td> <td>   -0.609</td> <td> 0.544</td> <td>   -0.241</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0492</td> <td>    0.099</td> <td>   -0.497</td> <td> 0.620</td> <td>   -0.245</td> <td>    0.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.2016</td> <td>    0.106</td> <td>    1.902</td> <td> 0.060</td> <td>   -0.008</td> <td>    0.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.0210</td> <td>    0.091</td> <td>    0.230</td> <td> 0.818</td> <td>   -0.160</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0241</td> <td>    0.094</td> <td>    0.257</td> <td> 0.798</td> <td>   -0.162</td> <td>    0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.0345</td> <td>    0.102</td> <td>   -0.338</td> <td> 0.736</td> <td>   -0.237</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.0140</td> <td>    0.115</td> <td>    0.121</td> <td> 0.904</td> <td>   -0.214</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.1275</td> <td>    0.107</td> <td>    1.190</td> <td> 0.236</td> <td>   -0.085</td> <td>    0.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0317</td> <td>    0.099</td> <td>    0.320</td> <td> 0.749</td> <td>   -0.164</td> <td>    0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.0502</td> <td>    0.098</td> <td>   -0.510</td> <td> 0.611</td> <td>   -0.245</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.1149</td> <td>    0.097</td> <td>    1.182</td> <td> 0.240</td> <td>   -0.078</td> <td>    0.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.0103</td> <td>    0.100</td> <td>   -0.102</td> <td> 0.919</td> <td>   -0.209</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0217</td> <td>    0.102</td> <td>    0.213</td> <td> 0.832</td> <td>   -0.180</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.0079</td> <td>    0.103</td> <td>    0.077</td> <td> 0.939</td> <td>   -0.196</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.1208</td> <td>    0.103</td> <td>   -1.173</td> <td> 0.243</td> <td>   -0.325</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>    0.0529</td> <td>    0.101</td> <td>    0.521</td> <td> 0.603</td> <td>   -0.148</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.1030</td> <td>    0.100</td> <td>   -1.035</td> <td> 0.303</td> <td>   -0.300</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.0099</td> <td>    0.096</td> <td>   -0.103</td> <td> 0.918</td> <td>   -0.200</td> <td>    0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>    0.0453</td> <td>    0.108</td> <td>    0.419</td> <td> 0.676</td> <td>   -0.169</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>   -0.0019</td> <td>    0.101</td> <td>   -0.019</td> <td> 0.985</td> <td>   -0.203</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>   -0.0052</td> <td>    0.107</td> <td>   -0.049</td> <td> 0.961</td> <td>   -0.218</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>   -0.1431</td> <td>    0.104</td> <td>   -1.375</td> <td> 0.172</td> <td>   -0.349</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    0.1341</td> <td>    0.099</td> <td>    1.351</td> <td> 0.179</td> <td>   -0.062</td> <td>    0.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.0251</td> <td>    0.097</td> <td>    0.260</td> <td> 0.796</td> <td>   -0.167</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>    0.0120</td> <td>    0.092</td> <td>    0.131</td> <td> 0.896</td> <td>   -0.169</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.0773</td> <td>    0.103</td> <td>    0.752</td> <td> 0.454</td> <td>   -0.126</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0106</td> <td>    0.102</td> <td>    0.103</td> <td> 0.918</td> <td>   -0.192</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -0.0606</td> <td>    0.093</td> <td>   -0.652</td> <td> 0.516</td> <td>   -0.245</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.0276</td> <td>    0.099</td> <td>    0.278</td> <td> 0.782</td> <td>   -0.169</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.1187</td> <td>    0.096</td> <td>    1.230</td> <td> 0.221</td> <td>   -0.072</td> <td>    0.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    0.0779</td> <td>    0.108</td> <td>    0.719</td> <td> 0.474</td> <td>   -0.137</td> <td>    0.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   -0.1649</td> <td>    0.101</td> <td>   -1.626</td> <td> 0.107</td> <td>   -0.366</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>    0.0065</td> <td>    0.108</td> <td>    0.061</td> <td> 0.952</td> <td>   -0.207</td> <td>    0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>    0.0638</td> <td>    0.104</td> <td>    0.613</td> <td> 0.541</td> <td>   -0.142</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>    0.0273</td> <td>    0.098</td> <td>    0.277</td> <td> 0.782</td> <td>   -0.168</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>    0.0519</td> <td>    0.099</td> <td>    0.522</td> <td> 0.603</td> <td>   -0.145</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    0.0496</td> <td>    0.102</td> <td>    0.484</td> <td> 0.629</td> <td>   -0.153</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    0.0173</td> <td>    0.099</td> <td>    0.174</td> <td> 0.862</td> <td>   -0.179</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   -0.1128</td> <td>    0.103</td> <td>   -1.094</td> <td> 0.276</td> <td>   -0.317</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>   -0.1070</td> <td>    0.106</td> <td>   -1.011</td> <td> 0.314</td> <td>   -0.316</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    0.0953</td> <td>    0.109</td> <td>    0.875</td> <td> 0.384</td> <td>   -0.120</td> <td>    0.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>   -0.0685</td> <td>    0.103</td> <td>   -0.663</td> <td> 0.508</td> <td>   -0.273</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>    0.0686</td> <td>    0.101</td> <td>    0.682</td> <td> 0.497</td> <td>   -0.131</td> <td>    0.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>    0.0135</td> <td>    0.102</td> <td>    0.133</td> <td> 0.895</td> <td>   -0.188</td> <td>    0.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>    0.0244</td> <td>    0.103</td> <td>    0.236</td> <td> 0.814</td> <td>   -0.180</td> <td>    0.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>    0.1826</td> <td>    0.101</td> <td>    1.805</td> <td> 0.074</td> <td>   -0.018</td> <td>    0.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>    0.0370</td> <td>    0.102</td> <td>    0.363</td> <td> 0.717</td> <td>   -0.165</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>    0.0225</td> <td>    0.098</td> <td>    0.229</td> <td> 0.819</td> <td>   -0.172</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>    0.1294</td> <td>    0.103</td> <td>    1.251</td> <td> 0.213</td> <td>   -0.075</td> <td>    0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>   -0.0712</td> <td>    0.096</td> <td>   -0.739</td> <td> 0.461</td> <td>   -0.262</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>    0.0056</td> <td>    0.101</td> <td>    0.056</td> <td> 0.956</td> <td>   -0.195</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>   -0.0130</td> <td>    0.108</td> <td>   -0.121</td> <td> 0.904</td> <td>   -0.226</td> <td>    0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>   -0.0981</td> <td>    0.105</td> <td>   -0.932</td> <td> 0.353</td> <td>   -0.307</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>   -0.1028</td> <td>    0.100</td> <td>   -1.032</td> <td> 0.304</td> <td>   -0.300</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>   -0.0317</td> <td>    0.101</td> <td>   -0.313</td> <td> 0.755</td> <td>   -0.232</td> <td>    0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>   -0.0334</td> <td>    0.106</td> <td>   -0.317</td> <td> 0.752</td> <td>   -0.242</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>   -0.0928</td> <td>    0.097</td> <td>   -0.953</td> <td> 0.343</td> <td>   -0.286</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   -0.0781</td> <td>    0.102</td> <td>   -0.766</td> <td> 0.445</td> <td>   -0.280</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>   -0.0055</td> <td>    0.107</td> <td>   -0.051</td> <td> 0.959</td> <td>   -0.217</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>    0.1345</td> <td>    0.096</td> <td>    1.397</td> <td> 0.165</td> <td>   -0.056</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>   -0.0187</td> <td>    0.100</td> <td>   -0.187</td> <td> 0.852</td> <td>   -0.217</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>   -0.1486</td> <td>    0.103</td> <td>   -1.439</td> <td> 0.153</td> <td>   -0.353</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>    0.0214</td> <td>    0.103</td> <td>    0.208</td> <td> 0.835</td> <td>   -0.182</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>    0.0832</td> <td>    0.104</td> <td>    0.803</td> <td> 0.424</td> <td>   -0.122</td> <td>    0.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>   -0.1163</td> <td>    0.104</td> <td>   -1.117</td> <td> 0.266</td> <td>   -0.323</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>   -0.0938</td> <td>    0.108</td> <td>   -0.870</td> <td> 0.386</td> <td>   -0.307</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>    0.0112</td> <td>    0.106</td> <td>    0.105</td> <td> 0.916</td> <td>   -0.199</td> <td>    0.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>    0.0931</td> <td>    0.096</td> <td>    0.974</td> <td> 0.332</td> <td>   -0.096</td> <td>    0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>   -0.0207</td> <td>    0.100</td> <td>   -0.208</td> <td> 0.836</td> <td>   -0.218</td> <td>    0.177</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.588</td> <th>  Durbin-Watson:     </th> <td>   2.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.452</td> <th>  Jarque-Bera (JB):  </th> <td>   1.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.101</td> <th>  Prob(JB):          </th> <td>   0.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.630</td> <th>  Cond. No.          </th> <td>    102.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.321\n",
       "Model:                            OLS   Adj. R-squared:                 -0.136\n",
       "Method:                 Least Squares   F-statistic:                    0.7021\n",
       "Date:                Thu, 12 Jul 2018   Prob (F-statistic):              0.954\n",
       "Time:                        09:16:34   Log-Likelihood:                -2.9463\n",
       "No. Observations:                 200   AIC:                             167.9\n",
       "Df Residuals:                     119   BIC:                             435.1\n",
       "Df Model:                          80                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3808      0.479      0.795      0.428      -0.568       1.329\n",
       "x1             0.0179      0.102      0.176      0.860      -0.183       0.219\n",
       "x2            -0.0327      0.104     -0.316      0.753      -0.238       0.172\n",
       "x3            -0.0615      0.104     -0.589      0.557      -0.268       0.145\n",
       "x4             0.1137      0.097      1.176      0.242      -0.078       0.305\n",
       "x5            -0.0642      0.104     -0.620      0.536      -0.269       0.141\n",
       "x6             0.0609      0.102      0.596      0.552      -0.141       0.263\n",
       "x7             0.0276      0.108      0.256      0.798      -0.186       0.241\n",
       "x8            -0.1214      0.103     -1.177      0.242      -0.326       0.083\n",
       "x9             0.0090      0.110      0.082      0.935      -0.208       0.226\n",
       "x10           -0.0049      0.094     -0.052      0.958      -0.192       0.182\n",
       "x11            0.0132      0.102      0.129      0.897      -0.189       0.215\n",
       "x12           -0.0567      0.093     -0.609      0.544      -0.241       0.128\n",
       "x13           -0.0492      0.099     -0.497      0.620      -0.245       0.147\n",
       "x14            0.2016      0.106      1.902      0.060      -0.008       0.411\n",
       "x15            0.0210      0.091      0.230      0.818      -0.160       0.202\n",
       "x16            0.0241      0.094      0.257      0.798      -0.162       0.210\n",
       "x17           -0.0345      0.102     -0.338      0.736      -0.237       0.168\n",
       "x18            0.0140      0.115      0.121      0.904      -0.214       0.242\n",
       "x19            0.1275      0.107      1.190      0.236      -0.085       0.340\n",
       "x20            0.0317      0.099      0.320      0.749      -0.164       0.228\n",
       "x21           -0.0502      0.098     -0.510      0.611      -0.245       0.145\n",
       "x22            0.1149      0.097      1.182      0.240      -0.078       0.307\n",
       "x23           -0.0103      0.100     -0.102      0.919      -0.209       0.189\n",
       "x24            0.0217      0.102      0.213      0.832      -0.180       0.224\n",
       "x25            0.0079      0.103      0.077      0.939      -0.196       0.211\n",
       "x26           -0.1208      0.103     -1.173      0.243      -0.325       0.083\n",
       "x27            0.0529      0.101      0.521      0.603      -0.148       0.254\n",
       "x28           -0.1030      0.100     -1.035      0.303      -0.300       0.094\n",
       "x29           -0.0099      0.096     -0.103      0.918      -0.200       0.181\n",
       "x30            0.0453      0.108      0.419      0.676      -0.169       0.259\n",
       "x31           -0.0019      0.101     -0.019      0.985      -0.203       0.199\n",
       "x32           -0.0052      0.107     -0.049      0.961      -0.218       0.207\n",
       "x33           -0.1431      0.104     -1.375      0.172      -0.349       0.063\n",
       "x34            0.1341      0.099      1.351      0.179      -0.062       0.331\n",
       "x35            0.0251      0.097      0.260      0.796      -0.167       0.217\n",
       "x36            0.0120      0.092      0.131      0.896      -0.169       0.193\n",
       "x37            0.0773      0.103      0.752      0.454      -0.126       0.281\n",
       "x38            0.0106      0.102      0.103      0.918      -0.192       0.213\n",
       "x39           -0.0606      0.093     -0.652      0.516      -0.245       0.124\n",
       "x40            0.0276      0.099      0.278      0.782      -0.169       0.225\n",
       "x41            0.1187      0.096      1.230      0.221      -0.072       0.310\n",
       "x42            0.0779      0.108      0.719      0.474      -0.137       0.293\n",
       "x43           -0.1649      0.101     -1.626      0.107      -0.366       0.036\n",
       "x44            0.0065      0.108      0.061      0.952      -0.207       0.220\n",
       "x45            0.0638      0.104      0.613      0.541      -0.142       0.270\n",
       "x46            0.0273      0.098      0.277      0.782      -0.168       0.222\n",
       "x47            0.0519      0.099      0.522      0.603      -0.145       0.249\n",
       "x48            0.0496      0.102      0.484      0.629      -0.153       0.252\n",
       "x49            0.0173      0.099      0.174      0.862      -0.179       0.214\n",
       "x50           -0.1128      0.103     -1.094      0.276      -0.317       0.091\n",
       "x51           -0.1070      0.106     -1.011      0.314      -0.316       0.102\n",
       "x52            0.0953      0.109      0.875      0.384      -0.120       0.311\n",
       "x53           -0.0685      0.103     -0.663      0.508      -0.273       0.136\n",
       "x54            0.0686      0.101      0.682      0.497      -0.131       0.268\n",
       "x55            0.0135      0.102      0.133      0.895      -0.188       0.216\n",
       "x56            0.0244      0.103      0.236      0.814      -0.180       0.229\n",
       "x57            0.1826      0.101      1.805      0.074      -0.018       0.383\n",
       "x58            0.0370      0.102      0.363      0.717      -0.165       0.239\n",
       "x59            0.0225      0.098      0.229      0.819      -0.172       0.217\n",
       "x60            0.1294      0.103      1.251      0.213      -0.075       0.334\n",
       "x61           -0.0712      0.096     -0.739      0.461      -0.262       0.120\n",
       "x62            0.0056      0.101      0.056      0.956      -0.195       0.206\n",
       "x63           -0.0130      0.108     -0.121      0.904      -0.226       0.200\n",
       "x64           -0.0981      0.105     -0.932      0.353      -0.307       0.110\n",
       "x65           -0.1028      0.100     -1.032      0.304      -0.300       0.094\n",
       "x66           -0.0317      0.101     -0.313      0.755      -0.232       0.169\n",
       "x67           -0.0334      0.106     -0.317      0.752      -0.242       0.176\n",
       "x68           -0.0928      0.097     -0.953      0.343      -0.286       0.100\n",
       "x69           -0.0781      0.102     -0.766      0.445      -0.280       0.124\n",
       "x70           -0.0055      0.107     -0.051      0.959      -0.217       0.206\n",
       "x71            0.1345      0.096      1.397      0.165      -0.056       0.325\n",
       "x72           -0.0187      0.100     -0.187      0.852      -0.217       0.179\n",
       "x73           -0.1486      0.103     -1.439      0.153      -0.353       0.056\n",
       "x74            0.0214      0.103      0.208      0.835      -0.182       0.224\n",
       "x75            0.0832      0.104      0.803      0.424      -0.122       0.288\n",
       "x76           -0.1163      0.104     -1.117      0.266      -0.323       0.090\n",
       "x77           -0.0938      0.108     -0.870      0.386      -0.307       0.120\n",
       "x78            0.0112      0.106      0.105      0.916      -0.199       0.221\n",
       "x79            0.0931      0.096      0.974      0.332      -0.096       0.282\n",
       "x80           -0.0207      0.100     -0.208      0.836      -0.218       0.177\n",
       "==============================================================================\n",
       "Omnibus:                        1.588   Durbin-Watson:                   2.100\n",
       "Prob(Omnibus):                  0.452   Jarque-Bera (JB):                1.482\n",
       "Skew:                          -0.101   Prob(JB):                        0.477\n",
       "Kurtosis:                       2.630   Cond. No.                         102.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X80 = np.random.rand(200, 80)\n",
    "model80 = sm.OLS(y, sm.add_constant(X80))\n",
    "# Fit your model to your training set\n",
    "fit80 = model80.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit80.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.8416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 12 Jul 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.805</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:17:19</td>     <th>  Log-Likelihood:    </th> <td>  19.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   162.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    99</td>      <th>  BIC:               </th> <td>   495.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   100</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.9350</td> <td>    0.551</td> <td>    1.698</td> <td> 0.093</td> <td>   -0.157</td> <td>    2.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0778</td> <td>    0.099</td> <td>    0.787</td> <td> 0.433</td> <td>   -0.118</td> <td>    0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.1678</td> <td>    0.114</td> <td>   -1.469</td> <td> 0.145</td> <td>   -0.394</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0363</td> <td>    0.110</td> <td>   -0.329</td> <td> 0.743</td> <td>   -0.255</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0132</td> <td>    0.113</td> <td>    0.117</td> <td> 0.907</td> <td>   -0.210</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0075</td> <td>    0.112</td> <td>    0.067</td> <td> 0.947</td> <td>   -0.215</td> <td>    0.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.1322</td> <td>    0.122</td> <td>   -1.080</td> <td> 0.283</td> <td>   -0.375</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.1998</td> <td>    0.104</td> <td>    1.922</td> <td> 0.058</td> <td>   -0.006</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.1984</td> <td>    0.125</td> <td>   -1.583</td> <td> 0.117</td> <td>   -0.447</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -0.0712</td> <td>    0.096</td> <td>   -0.744</td> <td> 0.458</td> <td>   -0.261</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0337</td> <td>    0.103</td> <td>    0.329</td> <td> 0.743</td> <td>   -0.170</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.1971</td> <td>    0.116</td> <td>   -1.698</td> <td> 0.093</td> <td>   -0.427</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.0893</td> <td>    0.095</td> <td>    0.940</td> <td> 0.349</td> <td>   -0.099</td> <td>    0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0429</td> <td>    0.103</td> <td>   -0.416</td> <td> 0.679</td> <td>   -0.248</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.0038</td> <td>    0.106</td> <td>    0.036</td> <td> 0.972</td> <td>   -0.206</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.1013</td> <td>    0.106</td> <td>   -0.954</td> <td> 0.342</td> <td>   -0.312</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.0511</td> <td>    0.106</td> <td>   -0.482</td> <td> 0.631</td> <td>   -0.261</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.0601</td> <td>    0.103</td> <td>    0.585</td> <td> 0.560</td> <td>   -0.144</td> <td>    0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.2212</td> <td>    0.120</td> <td>   -1.843</td> <td> 0.068</td> <td>   -0.459</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0221</td> <td>    0.105</td> <td>    0.210</td> <td> 0.834</td> <td>   -0.187</td> <td>    0.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.0917</td> <td>    0.101</td> <td>   -0.907</td> <td> 0.366</td> <td>   -0.292</td> <td>    0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.0490</td> <td>    0.109</td> <td>    0.451</td> <td> 0.653</td> <td>   -0.166</td> <td>    0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.0538</td> <td>    0.114</td> <td>    0.471</td> <td> 0.639</td> <td>   -0.173</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.0192</td> <td>    0.112</td> <td>   -0.173</td> <td> 0.863</td> <td>   -0.241</td> <td>    0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0540</td> <td>    0.109</td> <td>    0.495</td> <td> 0.621</td> <td>   -0.162</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.0329</td> <td>    0.104</td> <td>    0.316</td> <td> 0.752</td> <td>   -0.173</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.1879</td> <td>    0.109</td> <td>   -1.726</td> <td> 0.088</td> <td>   -0.404</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.1102</td> <td>    0.103</td> <td>   -1.073</td> <td> 0.286</td> <td>   -0.314</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.1479</td> <td>    0.105</td> <td>   -1.403</td> <td> 0.164</td> <td>   -0.357</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.2515</td> <td>    0.117</td> <td>   -2.155</td> <td> 0.034</td> <td>   -0.483</td> <td>   -0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.1225</td> <td>    0.100</td> <td>   -1.221</td> <td> 0.225</td> <td>   -0.322</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>   -0.0242</td> <td>    0.109</td> <td>   -0.223</td> <td> 0.824</td> <td>   -0.240</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>   -0.0450</td> <td>    0.107</td> <td>   -0.422</td> <td> 0.674</td> <td>   -0.257</td> <td>    0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>   -0.0926</td> <td>    0.107</td> <td>   -0.864</td> <td> 0.390</td> <td>   -0.305</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -0.0540</td> <td>    0.109</td> <td>   -0.496</td> <td> 0.621</td> <td>   -0.270</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>   -0.1515</td> <td>    0.113</td> <td>   -1.339</td> <td> 0.184</td> <td>   -0.376</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   -0.0175</td> <td>    0.111</td> <td>   -0.157</td> <td> 0.875</td> <td>   -0.238</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -0.1966</td> <td>    0.105</td> <td>   -1.864</td> <td> 0.065</td> <td>   -0.406</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0990</td> <td>    0.115</td> <td>    0.862</td> <td> 0.391</td> <td>   -0.129</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -0.0873</td> <td>    0.097</td> <td>   -0.904</td> <td> 0.368</td> <td>   -0.279</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.0557</td> <td>    0.102</td> <td>    0.547</td> <td> 0.585</td> <td>   -0.146</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.0021</td> <td>    0.114</td> <td>    0.019</td> <td> 0.985</td> <td>   -0.223</td> <td>    0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    0.0206</td> <td>    0.103</td> <td>    0.201</td> <td> 0.841</td> <td>   -0.184</td> <td>    0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   -0.0075</td> <td>    0.108</td> <td>   -0.070</td> <td> 0.944</td> <td>   -0.222</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>    0.1123</td> <td>    0.096</td> <td>    1.164</td> <td> 0.247</td> <td>   -0.079</td> <td>    0.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.0502</td> <td>    0.108</td> <td>   -0.463</td> <td> 0.644</td> <td>   -0.265</td> <td>    0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -0.0415</td> <td>    0.105</td> <td>   -0.394</td> <td> 0.695</td> <td>   -0.251</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>    0.0322</td> <td>    0.107</td> <td>    0.302</td> <td> 0.763</td> <td>   -0.179</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    0.1429</td> <td>    0.104</td> <td>    1.377</td> <td> 0.172</td> <td>   -0.063</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    0.0283</td> <td>    0.109</td> <td>    0.260</td> <td> 0.795</td> <td>   -0.188</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   -0.0465</td> <td>    0.111</td> <td>   -0.418</td> <td> 0.677</td> <td>   -0.267</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>    0.0307</td> <td>    0.108</td> <td>    0.283</td> <td> 0.778</td> <td>   -0.185</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    0.1399</td> <td>    0.105</td> <td>    1.326</td> <td> 0.188</td> <td>   -0.069</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.0504</td> <td>    0.118</td> <td>    0.429</td> <td> 0.669</td> <td>   -0.183</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>    0.0723</td> <td>    0.105</td> <td>    0.687</td> <td> 0.494</td> <td>   -0.137</td> <td>    0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>    0.0122</td> <td>    0.121</td> <td>    0.101</td> <td> 0.920</td> <td>   -0.228</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>    0.1947</td> <td>    0.112</td> <td>    1.746</td> <td> 0.084</td> <td>   -0.027</td> <td>    0.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>   -0.0342</td> <td>    0.101</td> <td>   -0.339</td> <td> 0.735</td> <td>   -0.235</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>   -0.0573</td> <td>    0.117</td> <td>   -0.488</td> <td> 0.627</td> <td>   -0.290</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>    0.1633</td> <td>    0.106</td> <td>    1.544</td> <td> 0.126</td> <td>   -0.047</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>   -0.0687</td> <td>    0.121</td> <td>   -0.567</td> <td> 0.572</td> <td>   -0.309</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>    0.0418</td> <td>    0.109</td> <td>    0.385</td> <td> 0.701</td> <td>   -0.174</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>   -0.0848</td> <td>    0.108</td> <td>   -0.784</td> <td> 0.435</td> <td>   -0.299</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>    0.2817</td> <td>    0.117</td> <td>    2.413</td> <td> 0.018</td> <td>    0.050</td> <td>    0.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>    0.2118</td> <td>    0.113</td> <td>    1.870</td> <td> 0.064</td> <td>   -0.013</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>    0.0954</td> <td>    0.113</td> <td>    0.848</td> <td> 0.399</td> <td>   -0.128</td> <td>    0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>    0.1262</td> <td>    0.100</td> <td>    1.259</td> <td> 0.211</td> <td>   -0.073</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>   -0.0914</td> <td>    0.112</td> <td>   -0.819</td> <td> 0.415</td> <td>   -0.313</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>   -0.1776</td> <td>    0.105</td> <td>   -1.694</td> <td> 0.093</td> <td>   -0.386</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>   -0.0628</td> <td>    0.107</td> <td>   -0.586</td> <td> 0.559</td> <td>   -0.276</td> <td>    0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>   -0.0189</td> <td>    0.105</td> <td>   -0.180</td> <td> 0.858</td> <td>   -0.227</td> <td>    0.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>    0.0518</td> <td>    0.106</td> <td>    0.489</td> <td> 0.626</td> <td>   -0.158</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>    0.1543</td> <td>    0.100</td> <td>    1.543</td> <td> 0.126</td> <td>   -0.044</td> <td>    0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>    0.1513</td> <td>    0.118</td> <td>    1.280</td> <td> 0.203</td> <td>   -0.083</td> <td>    0.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>   -0.0733</td> <td>    0.126</td> <td>   -0.581</td> <td> 0.563</td> <td>   -0.324</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>   -0.0328</td> <td>    0.112</td> <td>   -0.291</td> <td> 0.771</td> <td>   -0.256</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>   -0.0689</td> <td>    0.109</td> <td>   -0.635</td> <td> 0.527</td> <td>   -0.284</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>   -0.0561</td> <td>    0.105</td> <td>   -0.534</td> <td> 0.595</td> <td>   -0.265</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>    0.1392</td> <td>    0.116</td> <td>    1.205</td> <td> 0.231</td> <td>   -0.090</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>   -0.0635</td> <td>    0.102</td> <td>   -0.620</td> <td> 0.537</td> <td>   -0.267</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>   -0.0104</td> <td>    0.109</td> <td>   -0.096</td> <td> 0.924</td> <td>   -0.227</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>    0.0798</td> <td>    0.102</td> <td>    0.783</td> <td> 0.435</td> <td>   -0.122</td> <td>    0.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>   -0.1239</td> <td>    0.105</td> <td>   -1.185</td> <td> 0.239</td> <td>   -0.331</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>    0.0389</td> <td>    0.105</td> <td>    0.369</td> <td> 0.713</td> <td>   -0.170</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>    0.0807</td> <td>    0.106</td> <td>    0.760</td> <td> 0.449</td> <td>   -0.130</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td>    0.1623</td> <td>    0.114</td> <td>    1.420</td> <td> 0.159</td> <td>   -0.065</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>   -0.1124</td> <td>    0.098</td> <td>   -1.151</td> <td> 0.252</td> <td>   -0.306</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td>    0.1020</td> <td>    0.108</td> <td>    0.945</td> <td> 0.347</td> <td>   -0.112</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>    0.0014</td> <td>    0.119</td> <td>    0.012</td> <td> 0.990</td> <td>   -0.235</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>   -0.0086</td> <td>    0.112</td> <td>   -0.077</td> <td> 0.939</td> <td>   -0.230</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>   -0.0135</td> <td>    0.111</td> <td>   -0.121</td> <td> 0.904</td> <td>   -0.234</td> <td>    0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>    0.1956</td> <td>    0.104</td> <td>    1.886</td> <td> 0.062</td> <td>   -0.010</td> <td>    0.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>    0.0622</td> <td>    0.101</td> <td>    0.617</td> <td> 0.538</td> <td>   -0.138</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>   -0.0827</td> <td>    0.115</td> <td>   -0.719</td> <td> 0.474</td> <td>   -0.311</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>   -0.1871</td> <td>    0.112</td> <td>   -1.664</td> <td> 0.099</td> <td>   -0.410</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>   -0.0077</td> <td>    0.098</td> <td>   -0.079</td> <td> 0.937</td> <td>   -0.201</td> <td>    0.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>   -0.2002</td> <td>    0.102</td> <td>   -1.968</td> <td> 0.052</td> <td>   -0.402</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>    0.1287</td> <td>    0.111</td> <td>    1.160</td> <td> 0.249</td> <td>   -0.091</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>   -0.1095</td> <td>    0.108</td> <td>   -1.014</td> <td> 0.313</td> <td>   -0.324</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td>   -0.0476</td> <td>    0.125</td> <td>   -0.380</td> <td> 0.704</td> <td>   -0.296</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td>   -0.0719</td> <td>    0.110</td> <td>   -0.656</td> <td> 0.513</td> <td>   -0.290</td> <td>    0.146</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 8.641</td> <th>  Durbin-Watson:     </th> <td>   2.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.013</td> <th>  Jarque-Bera (JB):  </th> <td>   4.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.140</td> <th>  Prob(JB):          </th> <td>   0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.318</td> <th>  Cond. No.          </th> <td>    133.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.459\n",
       "Model:                            OLS   Adj. R-squared:                 -0.086\n",
       "Method:                 Least Squares   F-statistic:                    0.8416\n",
       "Date:                Thu, 12 Jul 2018   Prob (F-statistic):              0.805\n",
       "Time:                        09:17:19   Log-Likelihood:                 19.918\n",
       "No. Observations:                 200   AIC:                             162.2\n",
       "Df Residuals:                      99   BIC:                             495.3\n",
       "Df Model:                         100                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.9350      0.551      1.698      0.093      -0.157       2.027\n",
       "x1             0.0778      0.099      0.787      0.433      -0.118       0.274\n",
       "x2            -0.1678      0.114     -1.469      0.145      -0.394       0.059\n",
       "x3            -0.0363      0.110     -0.329      0.743      -0.255       0.183\n",
       "x4             0.0132      0.113      0.117      0.907      -0.210       0.237\n",
       "x5             0.0075      0.112      0.067      0.947      -0.215       0.230\n",
       "x6            -0.1322      0.122     -1.080      0.283      -0.375       0.111\n",
       "x7             0.1998      0.104      1.922      0.058      -0.006       0.406\n",
       "x8            -0.1984      0.125     -1.583      0.117      -0.447       0.050\n",
       "x9            -0.0712      0.096     -0.744      0.458      -0.261       0.119\n",
       "x10            0.0337      0.103      0.329      0.743      -0.170       0.237\n",
       "x11           -0.1971      0.116     -1.698      0.093      -0.427       0.033\n",
       "x12            0.0893      0.095      0.940      0.349      -0.099       0.278\n",
       "x13           -0.0429      0.103     -0.416      0.679      -0.248       0.162\n",
       "x14            0.0038      0.106      0.036      0.972      -0.206       0.213\n",
       "x15           -0.1013      0.106     -0.954      0.342      -0.312       0.109\n",
       "x16           -0.0511      0.106     -0.482      0.631      -0.261       0.159\n",
       "x17            0.0601      0.103      0.585      0.560      -0.144       0.264\n",
       "x18           -0.2212      0.120     -1.843      0.068      -0.459       0.017\n",
       "x19            0.0221      0.105      0.210      0.834      -0.187       0.231\n",
       "x20           -0.0917      0.101     -0.907      0.366      -0.292       0.109\n",
       "x21            0.0490      0.109      0.451      0.653      -0.166       0.264\n",
       "x22            0.0538      0.114      0.471      0.639      -0.173       0.281\n",
       "x23           -0.0192      0.112     -0.173      0.863      -0.241       0.202\n",
       "x24            0.0540      0.109      0.495      0.621      -0.162       0.270\n",
       "x25            0.0329      0.104      0.316      0.752      -0.173       0.239\n",
       "x26           -0.1879      0.109     -1.726      0.088      -0.404       0.028\n",
       "x27           -0.1102      0.103     -1.073      0.286      -0.314       0.094\n",
       "x28           -0.1479      0.105     -1.403      0.164      -0.357       0.061\n",
       "x29           -0.2515      0.117     -2.155      0.034      -0.483      -0.020\n",
       "x30           -0.1225      0.100     -1.221      0.225      -0.322       0.077\n",
       "x31           -0.0242      0.109     -0.223      0.824      -0.240       0.191\n",
       "x32           -0.0450      0.107     -0.422      0.674      -0.257       0.167\n",
       "x33           -0.0926      0.107     -0.864      0.390      -0.305       0.120\n",
       "x34           -0.0540      0.109     -0.496      0.621      -0.270       0.162\n",
       "x35           -0.1515      0.113     -1.339      0.184      -0.376       0.073\n",
       "x36           -0.0175      0.111     -0.157      0.875      -0.238       0.203\n",
       "x37           -0.1966      0.105     -1.864      0.065      -0.406       0.013\n",
       "x38            0.0990      0.115      0.862      0.391      -0.129       0.327\n",
       "x39           -0.0873      0.097     -0.904      0.368      -0.279       0.104\n",
       "x40            0.0557      0.102      0.547      0.585      -0.146       0.258\n",
       "x41            0.0021      0.114      0.019      0.985      -0.223       0.228\n",
       "x42            0.0206      0.103      0.201      0.841      -0.184       0.225\n",
       "x43           -0.0075      0.108     -0.070      0.944      -0.222       0.207\n",
       "x44            0.1123      0.096      1.164      0.247      -0.079       0.304\n",
       "x45           -0.0502      0.108     -0.463      0.644      -0.265       0.165\n",
       "x46           -0.0415      0.105     -0.394      0.695      -0.251       0.168\n",
       "x47            0.0322      0.107      0.302      0.763      -0.179       0.244\n",
       "x48            0.1429      0.104      1.377      0.172      -0.063       0.349\n",
       "x49            0.0283      0.109      0.260      0.795      -0.188       0.244\n",
       "x50           -0.0465      0.111     -0.418      0.677      -0.267       0.174\n",
       "x51            0.0307      0.108      0.283      0.778      -0.185       0.246\n",
       "x52            0.1399      0.105      1.326      0.188      -0.069       0.349\n",
       "x53            0.0504      0.118      0.429      0.669      -0.183       0.284\n",
       "x54            0.0723      0.105      0.687      0.494      -0.137       0.281\n",
       "x55            0.0122      0.121      0.101      0.920      -0.228       0.253\n",
       "x56            0.1947      0.112      1.746      0.084      -0.027       0.416\n",
       "x57           -0.0342      0.101     -0.339      0.735      -0.235       0.166\n",
       "x58           -0.0573      0.117     -0.488      0.627      -0.290       0.176\n",
       "x59            0.1633      0.106      1.544      0.126      -0.047       0.373\n",
       "x60           -0.0687      0.121     -0.567      0.572      -0.309       0.172\n",
       "x61            0.0418      0.109      0.385      0.701      -0.174       0.257\n",
       "x62           -0.0848      0.108     -0.784      0.435      -0.299       0.130\n",
       "x63            0.2817      0.117      2.413      0.018       0.050       0.513\n",
       "x64            0.2118      0.113      1.870      0.064      -0.013       0.436\n",
       "x65            0.0954      0.113      0.848      0.399      -0.128       0.319\n",
       "x66            0.1262      0.100      1.259      0.211      -0.073       0.325\n",
       "x67           -0.0914      0.112     -0.819      0.415      -0.313       0.130\n",
       "x68           -0.1776      0.105     -1.694      0.093      -0.386       0.030\n",
       "x69           -0.0628      0.107     -0.586      0.559      -0.276       0.150\n",
       "x70           -0.0189      0.105     -0.180      0.858      -0.227       0.189\n",
       "x71            0.0518      0.106      0.489      0.626      -0.158       0.262\n",
       "x72            0.1543      0.100      1.543      0.126      -0.044       0.353\n",
       "x73            0.1513      0.118      1.280      0.203      -0.083       0.386\n",
       "x74           -0.0733      0.126     -0.581      0.563      -0.324       0.177\n",
       "x75           -0.0328      0.112     -0.291      0.771      -0.256       0.190\n",
       "x76           -0.0689      0.109     -0.635      0.527      -0.284       0.146\n",
       "x77           -0.0561      0.105     -0.534      0.595      -0.265       0.152\n",
       "x78            0.1392      0.116      1.205      0.231      -0.090       0.368\n",
       "x79           -0.0635      0.102     -0.620      0.537      -0.267       0.140\n",
       "x80           -0.0104      0.109     -0.096      0.924      -0.227       0.206\n",
       "x81            0.0798      0.102      0.783      0.435      -0.122       0.282\n",
       "x82           -0.1239      0.105     -1.185      0.239      -0.331       0.084\n",
       "x83            0.0389      0.105      0.369      0.713      -0.170       0.248\n",
       "x84            0.0807      0.106      0.760      0.449      -0.130       0.291\n",
       "x85            0.1623      0.114      1.420      0.159      -0.065       0.389\n",
       "x86           -0.1124      0.098     -1.151      0.252      -0.306       0.081\n",
       "x87            0.1020      0.108      0.945      0.347      -0.112       0.316\n",
       "x88            0.0014      0.119      0.012      0.990      -0.235       0.238\n",
       "x89           -0.0086      0.112     -0.077      0.939      -0.230       0.213\n",
       "x90           -0.0135      0.111     -0.121      0.904      -0.234       0.207\n",
       "x91            0.1956      0.104      1.886      0.062      -0.010       0.401\n",
       "x92            0.0622      0.101      0.617      0.538      -0.138       0.262\n",
       "x93           -0.0827      0.115     -0.719      0.474      -0.311       0.146\n",
       "x94           -0.1871      0.112     -1.664      0.099      -0.410       0.036\n",
       "x95           -0.0077      0.098     -0.079      0.937      -0.201       0.186\n",
       "x96           -0.2002      0.102     -1.968      0.052      -0.402       0.002\n",
       "x97            0.1287      0.111      1.160      0.249      -0.091       0.349\n",
       "x98           -0.1095      0.108     -1.014      0.313      -0.324       0.105\n",
       "x99           -0.0476      0.125     -0.380      0.704      -0.296       0.201\n",
       "x100          -0.0719      0.110     -0.656      0.513      -0.290       0.146\n",
       "==============================================================================\n",
       "Omnibus:                        8.641   Durbin-Watson:                   2.178\n",
       "Prob(Omnibus):                  0.013   Jarque-Bera (JB):                4.538\n",
       "Skew:                          -0.140   Prob(JB):                        0.103\n",
       "Kurtosis:                       2.318   Cond. No.                         133.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X100 = np.random.rand(200, 100)\n",
    "model100 = sm.OLS(y, sm.add_constant(X100))\n",
    "# Fit your model to your training set\n",
    "fit100 = model100.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit100.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.arange(1,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelRandomNoice(features, sample_size):\n",
    "    y = np.random.rand(sample_size)\n",
    "    result = np.zeros((len(features), 2))\n",
    "    for i in range(len(features)):\n",
    "        X = np.random.rand(sample_size, features[i])\n",
    "        model = sm.OLS(y, sm.add_constant(X))\n",
    "        fit = model.fit()\n",
    "        result[i,] = [fit.rsquared, fit.rsquared_adj]\n",
    "        # print(fit.summary)\n",
    "    plt.plot(features, result[:,0], 'g', label = 'rSquared')\n",
    "    plt.plot(features, result[:,1], 'r', label = 'rSquared-adj')\n",
    "    plt.legend()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1ff61f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = modelRandomNoice(features, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06474717, -0.03975035],\n",
       "       [ 0.1848683 , -0.02019628],\n",
       "       [ 0.35027468,  0.06981771],\n",
       "       [ 0.3631952 , -0.06490887],\n",
       "       [ 0.48447788, -0.03625153]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d+f2+g3+h4+j5'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
