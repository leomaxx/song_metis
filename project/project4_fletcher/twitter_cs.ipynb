{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project plan**\n",
    "\n",
    "*Data preprocessing\n",
    "1. DONE - Detect the english tweets\n",
    "https://github.com/rfk/pyenchant\n",
    "\n",
    "*Sentiment Analysis*\n",
    "1. Find a sentiment analysis NN to rate last tweets\n",
    "2. DONE - Extract the emoji's and incorporate the emoji sentiment <br>\n",
    "https://stackoverflow.com/questions/43852668/using-collections-counter-to-count-emojis-with-different-colors<br>\n",
    "https://stackoverflow.com/questions/48340622/extract-all-emojis-from-string-and-ignore-fitzpatrick-modifiers-skin-tones-etc\n",
    "\n",
    "*Build a pipeline for similar tweet recommender*\n",
    "1. DONE - Implement class for recommend engine\n",
    "2. Build a pipeline for word embedding, vectorizing, and recommendation\n",
    "3. Try out Glove, Google embedding.. etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import emoji\n",
    "import regex\n",
    "import pickle\n",
    "import re\n",
    "import enchant\n",
    "import math\n",
    "import inflect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vaderSentiment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import compress\n",
    "from tqdm import tqdm_notebook\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "import nltk.sentiment.vader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter = pd.read_csv(\"./customer-support-on-twitter/twcs.csv\")\n",
    "\n",
    "# Save original message for future recommendation display\n",
    "ticket_reserve = twitter[twitter['inbound'] == True]\n",
    "solution_reserve = twitter[twitter['inbound'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude tweets with non-English characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixContractions(text):\n",
    "    with open('./data/helper/contraction_list.pkl', 'rb') as picklefile:\n",
    "        cList = pickle.load(picklefile)\n",
    "    for word in text.split():\n",
    "        if word.lower() in cList:\n",
    "            text = text.replace(word, cList[word.lower()])\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https:\\/\\/t.co\\/\\w{10}', '', text) # Remove URL link\n",
    "    text = re.sub(r'@\\w+', '', text) # Remove @account\n",
    "    text = fixContractions(text) # Expand contractions\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emoji_free_text(text):\n",
    "    text = text.replace(u'\\u200d', '')\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    if (len(emoji_list) == 0):\n",
    "        return np.NaN, clean_text\n",
    "    else:\n",
    "        return emoji_list, clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter['text'] = twitter['text'].apply(lambda x:clean_text(x))\n",
    "twitter[['emoji','text']] = twitter['text'].apply(lambda x:pd.Series(get_emoji_free_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step1_emoji_extracted.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(twitter, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step1_emoji_extracted.pkl', 'rb')as picklefile:\n",
    "    twitter = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve sentiment score for emoji's\n",
    "ref:http://kt.ijs.si/data/Emoji_sentiment_ranking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(emoji_table, emoji_sentiment_dict):\n",
    "    type_set = set()\n",
    "    for i in range(emoji_table.shape[0]):\n",
    "        code = emoji_table.loc[i, 'Python Code']\n",
    "        try:\n",
    "            key = chr(int(code[1:], 16))\n",
    "            emoji_sentiment_dict[key] = emoji_table.loc[i, 'Sentiment score']\n",
    "        except:\n",
    "            type_set.add(emoji_table.loc[i, 'Unicode block'])\n",
    "    return emoji_sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emoji_sentiment(emoji_list):\n",
    "    score = 0\n",
    "    if (type(emoji_list) != float):\n",
    "        for emoji_item in emoji_list:\n",
    "            if (emoji_item in emoji_sentiment_dict.keys()):\n",
    "                score += emoji_sentiment_dict[emoji_item]\n",
    "        if (score == 0):\n",
    "            return np.NaN\n",
    "        else:\n",
    "            return score/len(emoji_list)\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets with emoji score:  161402\n",
      "Number of tweets with emoji but not score:  33223\n"
     ]
    }
   ],
   "source": [
    "emoji_sentiment = pd.read_csv('./data/raw/emoji_nodingbat.csv')\n",
    "emoji_other = pd.read_csv('./data/raw/emoji_other.csv')\n",
    "emoji_sentiment_dict = {}\n",
    "\n",
    "emoji_sentiment_dict = build_dict(emoji_sentiment, emoji_sentiment_dict)\n",
    "emoji_sentiment_dict = build_dict(emoji_other, emoji_sentiment_dict)\n",
    "\n",
    "twitter['emoji_score'] = twitter['emoji'].apply(lambda x:get_emoji_sentiment(x))\n",
    "print ('Number of tweets with emoji score: ', twitter[~twitter['emoji_score'].isnull()].shape[0])\n",
    "print ('Number of tweets with emoji but not score: ', twitter[twitter['emoji_score'].isnull() & (~twitter['emoji'].isnull())].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step2_emoji_sentiment.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(twitter, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step2_emoji_sentiment.pkl', 'rb')as picklefile:\n",
    "    twitter = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve elements that are printable in English context - This is a \n",
    "# combination of digits, letters, punctuation, and whitespace.\n",
    "valid_content = set(string.printable)\n",
    "\n",
    "# Include some common non-English punctuations\n",
    "non_English_punctuation = ['‘', '’', 'é', '–','—','“','”','、','。','`','️', '£', '…', '$']\n",
    "\n",
    "# Keep tweets with common non-English punctuations\n",
    "[valid_content.add(x) for x in (non_English_punctuation)]\n",
    "    \n",
    "def isEnglish(tweet):\n",
    "    invalid_index = [x not in valid_content for x in tweet]\n",
    "    if (np.sum(invalid_index)==0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "## Removed 78k tweets with non-English characters\n",
    "english_tweets = twitter.text.apply(lambda x:isEnglish(x))\n",
    "tweets = twitter.loc[english_tweets, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A quick tool to return non-English words\n",
    "def non_English(words):\n",
    "    print (list(compress(list(words), [x not in valid_content for x in words])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songlin/anaconda2/envs/Python36/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Format the dates\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'], format='%a %b %d %H:%M:%S +0000 %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songlin/anaconda2/envs/Python36/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/songlin/anaconda2/envs/Python36/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/songlin/anaconda2/envs/Python36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tweets['in_response_to_tweet_id'] = tweets['in_response_to_tweet_id'].fillna(-1)\n",
    "tweets['in_response_to_tweet_id'] = tweets['in_response_to_tweet_id'].astype(int)\n",
    "tweets['response_tweet_id'] = tweets['response_tweet_id'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step3_english.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(tweets, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step3_english.pkl', 'rb')as picklefile:\n",
    "    tweets = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add custom_id and brand_id columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add custom_id column\n",
    "tweets = pd.merge(tweets,pd.DataFrame(tweets[['tweet_id', 'author_id']]),left_on='in_response_to_tweet_id', right_on='tweet_id', how='left')\n",
    "tweets.loc[tweets['inbound'] == True, 'author_id_y'] = tweets.loc[tweets['inbound'] == True, 'author_id_x']\n",
    "tweets['author_id_y'] = tweets['author_id_y'].fillna(-1)\n",
    "tweets.drop('tweet_id_y', axis=1, inplace=True)\n",
    "tweets.rename(columns={'author_id_y':'cust_id', \n",
    "                       'author_id_x':'author_id',\n",
    "                      'tweet_id_x':'tweet_id'}, inplace=True)\n",
    "\n",
    "# Drop the tweets without customer id, those are usually promotional tweets initiated by\n",
    "# brands agent or customers not in the datase: Number of such tweets: 71255\n",
    "tweets = tweets[~tweets['cust_id'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add brand_id column\n",
    "\n",
    "# Carve out the first tweet id when there are multiples in response_tweet_id\n",
    "tweets['processed_response_id'] = tweets['response_tweet_id'].apply(\n",
    "    lambda x: x if (isinstance(x,int)) else x.split(',')[0])\n",
    "tweets['processed_response_id'] = tweets['processed_response_id'].astype(int)\n",
    "\n",
    "# Retrieve the author id of the responde tweet\n",
    "tweets = pd.merge(tweets,pd.DataFrame(tweets[['tweet_id', 'author_id']]),\n",
    "                  left_on='processed_response_id', \n",
    "                  right_on='tweet_id', \n",
    "                  how='left')\n",
    "\n",
    "tweets.drop('tweet_id_y', axis=1, inplace=True)\n",
    "tweets.rename(columns={'author_id_y':'brand_id', \n",
    "                       'author_id_x':'author_id',\n",
    "                      'tweet_id_x':'tweet_id'}, inplace=True)\n",
    "\n",
    "tweets = pd.merge(tweets,pd.DataFrame(tweets[['tweet_id', 'author_id']]),left_on='in_response_to_tweet_id', right_on='tweet_id', how='left')\n",
    "tweets.loc[(tweets['inbound'] & (tweets['processed_response_id'] == -1)), 'brand_id'] = (\n",
    "    tweets.loc[(tweets['inbound'] & (tweets['processed_response_id'] == -1)), 'author_id_y'])\n",
    "tweets.rename(columns={'tweet_id_x':'tweet_id', \n",
    "                       'author_id_x':'author_id'}, inplace=True)\n",
    "\n",
    "tweets.loc[tweets['inbound'] == False, 'brand_id'] = tweets.loc[tweets['inbound'] == False, 'author_id']\n",
    "tweets.drop(['processed_response_id', 'tweet_id_y', 'author_id_y'], axis=1, inplace=True)\n",
    "\n",
    "# could use some more logic to extract the account name in tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tweets with no response is 3880\n",
    "print (tweets[tweets['brand_id'].isnull() & (tweets['response_tweet_id'] == -1)].shape)\n",
    "\n",
    "# Tweets with response outside the dataset is 42764\n",
    "print (tweets[tweets['brand_id'].isnull() & (tweets['response_tweet_id'] != -1)].shape)\n",
    "\n",
    "# Drop the tweets without brand_id\n",
    "tweets = tweets[~tweets['brand_id'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify which tweets are in the same conversation (customer case)\n",
    "tweets.sort_values(['cust_id', 'created_at'], inplace=True)\n",
    "tweets.reset_index(inplace=True)\n",
    "\n",
    "# Calculate the time difference (in seconds) bwetween a tweet and its previous tweet\n",
    "tweets['time_diff'] = tweets['created_at'].diff()\n",
    "tweets['time_diff'] = tweets['time_diff'].apply(lambda x:x.total_seconds())\n",
    "tweets['time_diff'] = tweets['time_diff'].apply(lambda x: 0 if x <0 else x)\n",
    "tweets['time_diff'] = tweets['time_diff'].fillna(0)\n",
    "\n",
    "tweets['case_id'] = 0\n",
    "tweets['time_diff'] = tweets['time_diff'].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step4_brand_cust.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(tweets, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step4_brand_cust.pkl', 'rb')as picklefile:\n",
    "    tweets = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carve out the support data for a particular brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_brand(tweets, brand_cc_name):\n",
    "    ## Subset tweets according to brand\n",
    "    brand = tweets[tweets['brand_id'] == brand_cc_name]\n",
    "    brand.reset_index(inplace=True)\n",
    "    \n",
    "    ## Assign unique caseid for each conversation (interchange of tweets)\n",
    "    ## To avoid grouping different cases raised by the same customer at different time\n",
    "    ## We set the max threshold between subsequent tweets in a conversation as 2 days\n",
    "    caseid = 1\n",
    "    brand.loc[0, 'case_id'] = caseid\n",
    "    for row in range(1, brand.shape[0]):\n",
    "        if (row%1000 == 0):\n",
    "            print (row)\n",
    "        if ((brand.loc[row, 'cust_id'] != brand.loc[row-1, 'cust_id']) |\n",
    "            (brand.loc[row, 'time_diff'] > 3600*24*2)):\n",
    "            caseid = caseid + 1\n",
    "            brand.loc[row, 'time_diff'] = 0 # Set time between conversations as 0\n",
    "            brand.loc[row, 'case_id'] = caseid\n",
    "        else:\n",
    "            brand.loc[row, 'case_id'] = caseid\n",
    "    return brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songlin/anaconda2/envs/Python36/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n"
     ]
    }
   ],
   "source": [
    "# CAUTION! This function takes SUPER LONG TIME to run, 80k row takes 3 hours!\n",
    "spotify = analyze_brand(tweets, 'SpotifyCares')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/spotify.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(spotify, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/spotify.pkl', 'rb')as picklefile:\n",
    "    spotify = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove ticket without solution and solution without ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_count = spotify.groupby('case_id')['case_id'].count()\n",
    "single_conv_index = conv_count[conv_count==1].index.tolist()\n",
    "spotify['single_conv'] = spotify['case_id'].apply(\n",
    "    lambda x: True if x in single_conv_index else False)\n",
    "\n",
    "# Remove the conversations with only one tweet (424 ) \n",
    "spotify = spotify[~spotify['single_conv']]\n",
    "spotify.drop('single_conv', axis=1, inplace=True)\n",
    "\n",
    "spotify.drop(['level_0', 'index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticket = spotify[spotify['inbound'] == True]\n",
    "solution = spotify[spotify['inbound'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ticket without solution or solution without ticket \n",
    "# conversations with tweets from only customer or customer agents\n",
    "\n",
    "ticket = ticket.apply(\n",
    "    lambda x: x if (x[12] in solution['case_id'].unique()) else None, axis = 1)\n",
    "solution = solution.apply(\n",
    "    lambda x: x if (x[12] in ticket['case_id'].unique()) else None, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ticket.text.fillna('', inplace=True)\n",
    "#solution.text.fillna('', inplace=True)\n",
    "ticket_agg = ticket.groupby('case_id')['text'].agg(lambda x:' '.join(x))\n",
    "solution_agg = solution.groupby('case_id')['text'].agg(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify = spotify.apply(\n",
    "    lambda x: x if (x[10] in ticket['case_id'].unique()) else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/ticket.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(ticket, picklefile)\n",
    "with open('./data/processed/solution.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(solution, picklefile)\n",
    "with open('./data/processed/ticket_agg.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(ticket_agg, picklefile)\n",
    "with open('./data/processed/solution_agg.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(solution_agg, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform very basic sentiment analysis on last tweet from customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cust_last_tweet = ticket.groupby('case_id')[['text','emoji', 'emoji_score']].last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vdanalyzer = vaderSentiment.vaderSentiment.SentimentIntensityAnalyzer()\n",
    "cust_last_tweet['vader_sentiment'] = cust_last_tweet['text'].apply(\n",
    "        lambda x:vdanalyzer.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_analyzer = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
    "cust_last_tweet['ntlk_sentiment'] = cust_last_tweet['text'].apply(\n",
    "        lambda x:sentiment_analyzer.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_last_tweet.to_csv('./data/processed/processed_last_tweet_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_last_tweet = pd.read_csv('./data/processed/processed_last_tweet_sentiment.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6    taking features away will only make it easier for users to go. i’ve been a premium user for 4+ years... don’t let me leave.\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "print (ticket[ticket['tweet_id'] == 2606351.0].text.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_overall_sentiment_score(row):\n",
    "    sentiment = ''\n",
    "    if (not math.isnan(row[3])):\n",
    "        if (row[3] >= 0.2):\n",
    "            sentiment = 'POS'\n",
    "        elif ((row[3] < 0.2) & (row[3] > -0.05)):\n",
    "            sentiment = 'NEU'\n",
    "        elif (row[3] <= -0.05):\n",
    "            sentiment = 'NEG'\n",
    "    elif (not math.isnan(row[5])):\n",
    "        if (row[5] >= 0.2):\n",
    "            sentiment = 'POS'\n",
    "        elif ((row[5] < 0.2) & (row[5] > -0.05)):\n",
    "            sentiment = 'NEU'\n",
    "        elif (row[5] <= -0.05):\n",
    "            sentiment = 'NEG'\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cust_last_tweet[['emoji_score', 'ntlk_sentiment']] = cust_last_tweet[['emoji_score', 'ntlk_sentiment']].astype(float)\n",
    "cust_last_tweet['sentiment'] = cust_last_tweet.apply(lambda\n",
    "                        x:get_overall_sentiment_score(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_last_tweet.to_csv('./data/processed/processed_last_tweet_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cust_last_tweet = pd.read_csv('./data/processed/processed_last_tweet_sentiment.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spotify = pd.merge(spotify, cust_last_tweet[['case_id', 'sentiment']],\n",
    "                   left_on='case_id', right_on='case_id',\n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step5_spotify_sentiment.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(spotify, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_stopwords = ['http', 'https', 'spotify', 'help', 'hi', 'spotifycares']\n",
    "my_stop_words = (text.ENGLISH_STOP_WORDS.union(custom_stopwords)\n",
    "                 .union(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticket = pd.DataFrame(ticket_agg)\n",
    "df_ticket.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_solution = pd.DataFrame(solution_agg)\n",
    "df_solution.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticket = pd.merge(df_ticket, cust_last_tweet[['case_id', 'sentiment']], left_on='case_id', right_on='case_id',\n",
    "                   how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    clean_text = [lemmatize.lemmatize(token.lower().strip(), pos='a') for token in tokens]\n",
    "    clean_text = [x for x in clean_text if x not in my_stop_words]\n",
    "    return ' '.join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticket['text'] = df_ticket.text.apply(lambda x:text_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/tweets_step6_ticket_preprocessed.pkl', 'wb')as picklefile:\n",
    "    pickle.dump(df_ticket, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build recommendation engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read in files from previous steps\n",
    "with open('./data/processed/tweets_step6_ticket_preprocessed.pkl', 'rb')as picklefile:\n",
    "    df_ticket = pickle.load(picklefile)\n",
    "\n",
    "cust_last_tweet = pd.read_csv('./data/processed/processed_last_tweet_sentiment.csv', index_col=None)\n",
    "\n",
    "with open('./data/processed/ticket.pkl', 'rb')as picklefile:\n",
    "    ticket = pickle.dump(picklefile)\n",
    "\n",
    "with open('./data/processed/solution.pkl', 'rb')as picklefile:\n",
    "    solution = pickle.load(picklefile)\n",
    "    \n",
    "with open('./data/processed/ticket_agg.pkl', 'rb')as picklefile:\n",
    "    ticket_agg = pickle.dump(picklefile)\n",
    "\n",
    "with open('./data/processed/solution_agg.pkl', 'rb')as picklefile:\n",
    "    solution_agg = pickle.load(picklefile)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecommendationEngine:\n",
    "    def __init__(self, vectorizer, n_components, reducer, ticket, solution):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.n_dim = n_components\n",
    "        self.reducer = reducer(n_components)\n",
    "        self.ticket = ticket\n",
    "        self.solution = solution\n",
    "        \n",
    "    def fit(self, df):\n",
    "        self.vector_data = self.vectorizer.fit_transform(df['text'])\n",
    "        self.topic_data = self.reducer.fit_transform(self.vector_data)\n",
    "        self.df = df_ticket\n",
    "        return self.topic_data\n",
    "    \n",
    "    def _print_recommendations(self, article, rec_list):\n",
    "        p = inflect.engine()\n",
    "        print('NEW MESSAGE:', article)\n",
    "        i = 1\n",
    "        for resp in self.df.loc[rec_list]['case_id']:\n",
    "            print(f'\\n---{p.ordinal(i)} SIMILAR CONVERSATION---\\n')\n",
    "            self._display_conversation(resp)\n",
    "            print('\\n')\n",
    "            i += 1\n",
    "    \n",
    "    def _display_conversation (self, case_id):\n",
    "        cust_conv = self.ticket[self.ticket['case_id'] == case_id][['created_at', 'inbound', 'text']]\n",
    "        agent_conv = self.solution[self.solution['case_id'] == case_id][['created_at', 'inbound', 'text']]\n",
    "        conv_list = pd.concat([cust_conv, agent_conv]).sort_values('created_at')\n",
    "        for i in range(conv_list.shape[0]):\n",
    "            if (conv_list.iloc[i]['inbound']):\n",
    "                print ('CUSTOMER:   ', colored(conv_list.iloc[i]['text'], 'red'))\n",
    "            else:\n",
    "                print ('AGENT:      ', colored(conv_list.iloc[i]['text'], 'green'))\n",
    "        \n",
    "    def recommend(self, article, num_to_return):\n",
    "        article_vectorized = self.vectorizer.transform([article])\n",
    "        article_topic_vector = self.reducer.transform(article_vectorized)\n",
    "        nn = NearestNeighbors(n_neighbors=num_to_return, metric='cosine', algorithm='brute')\n",
    "        nn.fit(self.topic_data[self.resolved_mask])\n",
    "        results = nn.kneighbors(article_topic_vector)\n",
    "        #result_texts = [self.texts[i] for i in results[1][0]]\n",
    "        rec_list = results[1][0]\n",
    "        print ('number of results: ', len(rec_list))\n",
    "        self._print_recommendations(article, rec_list)\n",
    "        \n",
    "        return rec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticket_origin = pd.merge(ticket_reserve[['tweet_id', 'created_at', 'inbound', 'text']], ticket[['tweet_id', 'case_id']], left_on='tweet_id', right_on='tweet_id', how='right')\n",
    "solution_origin = pd.merge(solution_reserve[['tweet_id', 'created_at', 'inbound', 'text']], solution[['tweet_id', 'case_id']], left_on='tweet_id', right_on='tweet_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words=my_stop_words, \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6)\n",
    "\n",
    "\n",
    "#tfidf_data = tfidf_vectorizer.fit_transform(list(df_ticket.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resolved_ticket = df_ticket[df_ticket['sentiment'] == 'POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_1 = RecommendationEngine(tfidf_vectorizer, n_components = 300, reducer = TruncatedSVD, ticket = ticket_origin, solution=solution_origin)\n",
    "topic_data = engine_1.fit(df_ticket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28787, 300)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.94180905e-03, 0.00000000e+00, 1.57093790e-01, ...,\n",
       "        1.32147808e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "       [7.29738585e-02, 0.00000000e+00, 5.14639909e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [6.39817108e-04, 0.00000000e+00, 1.55882332e-01, ...,\n",
       "        0.00000000e+00, 1.71634402e-03, 0.00000000e+00],\n",
       "       ...,\n",
       "       [1.33665449e-04, 1.03610178e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.89677209e-02, 0.00000000e+00],\n",
       "       [1.33434110e-01, 1.92435947e-03, 5.68934551e-04, ...,\n",
       "        1.55217317e-02, 2.01762020e-03, 0.00000000e+00]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_2 = RecommendationEngine(count_vectorizer, n_components = 20, reducer = NMF, ticket = ticket_origin, solution=solution_origin)\n",
    "engine_2.fit(resolved_ticket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.08495029e-07, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.75172016e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.51387386e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.65453942e-03],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.75215421e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 9.09550852e-03]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_3 = RecommendationEngine(tfidf_vectorizer, n_components = 100, reducer = NMF, ticket = ticket_origin, solution=solution_origin)\n",
    "engine_3.fit(resolved_ticket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine_dict = {1:engine_1, 2:engine_2, 3:engine_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/processed/engines.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(engine_dict, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cust_first_tweet = ticket.groupby('case_id')[['text']].first().reset_index()\n",
    "cust_first_tweet.text[np.random.choice(cust_first_tweet.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of results:  3\n",
      "NEW MESSAGE: cool cool cool i think i am finally ready to switch to apple music\n",
      "\n",
      "---1st SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares Uh why is there a limit on how many songs I can \"save\" — how am I supposed to keep track of the albums I like? https://t.co/bk7Y7Jt93A\u001b[0m\n",
      "AGENT:       \u001b[32m@1084 Hey Nick! There's currently a 10k limit in Your Music (Songs + Albums + Artists). More info here: https://t.co/59ABtm5Ftl /MO\u001b[0m\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares cool cool cool I think I'm finally ready to switch to Apple Music\u001b[0m\n",
      "AGENT:       \u001b[32m@1084 Sorry to hear you feel that way. For now, you can vote for the idea here: https://t.co/8OOl8B2i69 and get support from other users /MO\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "---2nd SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@115888 You're cool, Spotify.\u001b[0m\n",
      "AGENT:       \u001b[32m@641342 Hey Alice! Thanks. You're not wrong, things sure are cooling down around the office. Brr... ⛄ https://t.co/uvDmDXjQFv\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "---3rd SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares this is cool\u001b[0m\n",
      "AGENT:       \u001b[32m@723983 Thanks for the feedback, Sara! It's music to our ears 🎶 If there's anything we can help with, feel free to send us a tweet. https://t.co/uQiQuWow27 🙂 /JE\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0, 21155, 24581])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_1.recommend(ticket.text[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of results:  3\n",
      "NEW MESSAGE: cool cool cool i think i am finally ready to switch to apple music\n",
      "\n",
      "---1st SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@115888 how dare you erase Julion Alvarez?!\u001b[0m\n",
      "AGENT:       \u001b[32m@382004 Hey Selene. Thanks for reaching out! There's some info on Spotify content here: https://t.co/0i8GpimuDa /MG\u001b[0m\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares But he’s extremely popular and talented... you need to get him back ASAP\u001b[0m\n",
      "AGENT:       \u001b[32m@382004 We do our best! But sometimes agreements are ongoing, or can't be reached with the content provider. Hopefully we'll have him back soon /RV\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "---2nd SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@115888 ok now its doing it to several albums by different artists including THIRD EYE BLIND's SELF TITLED CMON\u001b[0m\n",
      "AGENT:       \u001b[32m@329305 1: Hey, that's not cool. Can you send us the Song Links of the tracks affected? Just tap the three dots &gt; Share &gt; Copy Link.\u001b[0m\n",
      "AGENT:       \u001b[32m@329305 2: Also, what country is your account set to? We'll take a look backstage /JE\u001b[0m\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares US,\n",
      "\n",
      "https://t.co/J0UrCjAWu5\n",
      "\n",
      "https://t.co/d85YQwjHxR\u001b[0m\n",
      "AGENT:       \u001b[32m@329305 Got it! We can confirm those albums are available in the US. What device, operating system, and Spotify version are you using? /JE\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "---3rd SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares SOS I need to switch debit cards in the payment but I was just charged $25\u001b[0m\n",
      "AGENT:       \u001b[32m@249307 Hi Sara. Can you DM us your account's email address? We'll take a look backstage /LM https://t.co/ldFdZRiNAt\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10136,  8215,  4807])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_2.recommend(ticket.text[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of results:  3\n",
      "NEW MESSAGE: cool cool cool i think i am finally ready to switch to apple music\n",
      "\n",
      "---1st SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares Uh why is there a limit on how many songs I can \"save\" — how am I supposed to keep track of the albums I like? https://t.co/bk7Y7Jt93A\u001b[0m\n",
      "AGENT:       \u001b[32m@1084 Hey Nick! There's currently a 10k limit in Your Music (Songs + Albums + Artists). More info here: https://t.co/59ABtm5Ftl /MO\u001b[0m\n",
      "CUSTOMER:    \u001b[31m@SpotifyCares cool cool cool I think I'm finally ready to switch to Apple Music\u001b[0m\n",
      "AGENT:       \u001b[32m@1084 Sorry to hear you feel that way. For now, you can vote for the idea here: https://t.co/8OOl8B2i69 and get support from other users /MO\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "---2nd SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@115888 is there a way to remove an block devices from accessing my account because someone has connected there device to my account\u001b[0m\n",
      "AGENT:       \u001b[32m@346030 Hey there! We’re sorry to hear that. Check out https://t.co/QHn3ok8y2n for what to do next /BH\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "---3rd SIMILAR CONVERSATION---\n",
      "\n",
      "CUSTOMER:    \u001b[31m@115888 why can't you do this for metal, metal-core, hardcore, ect???? https://t.co/4HMjznIFNg\u001b[0m\n",
      "AGENT:       \u001b[32m@384535 Hey there! You might want to check this out: https://t.co/BjO8BUHUBF 🤘 /MU\u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,  8760, 10224])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_3.recommend(ticket.text[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-21 15:27:24,512 : INFO : loading projection weights from ~/Downloads/GoogleNews-vectors-negative300.bin\n",
      "2018-08-21 15:28:36,466 : INFO : loaded (3000000, 300) matrix from ~/Downloads/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "google_vec_file = '~/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to take a document as a list of words and return the document vector\n",
    "def get_doc_vec(words, model):\n",
    "    good_words = []\n",
    "    for word in words:\n",
    "        # Words not in the original model will fail\n",
    "        try:\n",
    "            if model.wv[word] is not None:\n",
    "                good_words.append(word)\n",
    "        except:\n",
    "            continue\n",
    "    # If no words are in the original model\n",
    "    if len(good_words) == 0:\n",
    "        return None\n",
    "    # Return the mean of the vectors for all the good words\n",
    "    return model.wv[good_words].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/songlin/anaconda2/envs/Python36/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n",
      "/Users/songlin/anaconda2/envs/Python36/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "ticket_google_vecs = resolved_ticket.text.apply(lambda x: get_doc_vec(x.split(), google_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 300)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model['the fat cat drank milk'.split()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda 3.6)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
